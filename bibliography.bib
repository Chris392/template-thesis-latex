@article{Jakobsen2017,
abstract = {Background: Missing data may seriously compromise inferences from randomised clinical trials, especially if missing data are not handled appropriately. The potential bias due to missing data depends on the mechanism causing the data to be missing, and the analytical methods applied to amend the missingness. Therefore, the analysis of trial data with missing values requires careful planning and attention. Methods: The authors had several meetings and discussions considering optimal ways of handling missing data to minimise the bias potential. We also searched PubMed (key words: missing data; randomi; statistical analysis) and reference lists of known studies for papers (theoretical papers; empirical studies; simulation studies; etc.) on how to deal with missing data when analysing randomised clinical trials. Results: Handling missing data is an important, yet difficult and complex task when analysing results of randomised clinical trials. We consider how to optimise the handling of missing data during the planning stage of a randomised clinical trial and recommend analytical approaches which may prevent bias caused by unavoidable missing data. We consider the strengths and limitations of using of best-worst and worst-best sensitivity analyses, multiple imputation, and full information maximum likelihood. We also present practical flowcharts on how to deal with missing data and an overview of the steps that always need to be considered during the analysis stage of a trial. Conclusions: We present a practical guide and flowcharts describing when and how multiple imputation should be used to handle missing data in randomised clinical.},
author = {Jakobsen, Janus Christian and Gluud, Christian and Wetterslev, J{\o}rn and Winkel, Per},
doi = {10.1186/s12874-017-0442-1},
file = {:C\:/Users/chris/Downloads/s12874-017-0442-1.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Missing data,Multiple imputation,Randomised clinical trials},
number = {1},
pages = {1--10},
pmid = {29207961},
publisher = {BMC Medical Research Methodology},
title = {{When and how should multiple imputation be used for handling missing data in randomised clinical trials - A practical guide with flowcharts}},
volume = {17},
year = {2017}
}
@article{Shrive2006,
abstract = {Background: Missing data present a challenge to many research projects. The problem is often pronounced in studies utilizing self-report scales, and literature addressing 
different strategies for dealing with missing data in such circumstances is scarce. The objective of this study was to compare six different imputation techniques for dealing with missing 
data in the Zung Self-reported Depression scale (SDS). Methods: 1580 participants from a surgical outcomes study completed the SDS. The SDS is a 20 question scale that respondents complete 
by circling a value of 1 to 4 for each question. The sum of the responses is calculated and respondents are classified as exhibiting depressive symptoms when their total score is over 40. 
Missing values were simulated by randomly selecting questions whose values were then deleted (a missing completely at random simulation). Additionally, a missing at random and missing not at 
random simulation were completed. Six imputation methods were then considered; 1) multiple imputation, 2) single regression, 3) individual mean, 4) overall mean, 5) participant's preceding 
response, and 6) random selection of a value from 1 to 4. For each method, the imputed mean SDS score and standard deviation were compared to the population statistics. The Spearman 
correlation coefficient, percent misclassified and the Kappa statistic were also calculated. Results: When 10 percent of values are missing, all the imputation methods except random selection 
produce Kappa statistics greater than 0.80 indicating 'near perfect' agreement. MI produces the most valid imputed values with a high Kappa statistic (0.89), although both single regression 
and individual mean imputation also produced favorable results. As the percent of missing information increased to 30 percent, or when unbalanced missing data were introduced, MI maintained a high 
Kappa statistic. The individual mean and single regression method produced Kappas in the 'substantial agreement' range (0.76 and 0.74 respectively). Conclusion: Multiple imputation is the 
most accurate method for dealing with missing data in most of the missind data scenarios we assessed for the SDS. Imputing the individual's mean is also an appropriate and simple method for 
dealing with missing data that may be more interpretable to the majority of medical readers. Researchers should consider conducting methodological assessments such as this one when 
confronted with missing data. The optimal method should balance validity, ease of interpretability for readers, and analysis expertise of the research team. © 2006 Shrive et al; licensee 
BioMed Central Ltd.},
author = {Shrive, Fiona M. and Stuart, Heather and Quan, Hude and Ghali, William A.},
doi = {10.1186/1471-2288-6-57},
issn = {14712288},
journal = {BMC Medical Research Methodology},
pages = {1--10},
pmid = {17166270},
title = {{Dealing with missing data in a multi-question depression scale: A comparison of imputation methods}},
volume = {6},
year = {2006}
}
@article{Hughes2019,
abstract = {Background: Missing data are unavoidable in epidemiological research, potentially leading to bias and loss of precision. Multiple imputation (MI) is widely advocated as an improvement over complete case analysis (CCA). However, contrary to widespread belief, CCA is preferable to MI in some situations. Methods: We provide guidance on choice of analysis when data are incomplete. Using causal diagrams to depict missingness mechanisms, we describe when CCA will not be biased by missing data and compare MI and CCA, with respect to bias and efficiency, in a range of missing data situations. We illustrate selection of an appropriate method in practice. Results: For most regression models, CCA gives unbiased results when the chance of being a complete case does not depend on the outcome after taking the covariates into consideration, which includes situations where data are missing not at random. Consequently, there are situations in which CCA analyses are unbiased while MI analyses, assuming missing at random (MAR), are biased. By contrast MI, unlike CCA, is valid for all MAR situations and has the potential to use information contained in the incomplete cases and auxiliary variables to reduce bias and/or improve precision. For this reason, MI was preferred over CCA in our real data example. Conclusions: Choice of method for dealing with missing data is crucial for validity of conclusions, and should be based on careful consideration of the reasons for the missing data, missing data patterns and the availability of auxiliary information.},
author = {Hughes, Rachael A. and Heron, Jon and Sterne, Jonathan A.C. and Tilling, Kate},
doi = {10.1093/ije/dyz032},
file = {:C\:/Users/chris/Downloads/dyz032.pdf:pdf},
issn = {14643685},
journal = {International Journal of Epidemiology},
keywords = {Complete case analysis,inverse probability weighting,missing data,missing data mechanisms,missing data patterns,multiple imputation},
number = {4},
pages = {1294--1304},
pmid = {30879056},
title = {{Accounting for missing data in statistical analyses: Multiple imputation is not always the answer}},
volume = {48},
year = {2019}
}
@article{Royston2005,
author = {Royston, Patrick},
doi = {10.1177/1536867x0500500404},
file = {:C\:/Users/chris/Downloads/1536867x0400400301.pdf:pdf},
issn = {1536867X},
journal = {Stata Journal},
keywords = {micombine,mijoin,misplit,missing at random,missing data,multiple imputation,multivariate imputation,mvis,regression modeling,st0067,uvis},
number = {4},
pages = {527--536},
title = {{Multiple imputation of missing values: Update of ice}},
volume = {5},
year = {2005}
}
@article{Donders2006,
abstract = {In most situations, simple techniques for handling missing data (such as complete case analysis, overall mean imputation, and the missing-indicator method) produce biased results, whereas imputation techniques yield valid results without complicating the analysis once the imputations are carried out. Imputation techniques are based on the idea that any subject in a study sample can be replaced by a new randomly chosen subject from the same source population. Imputation of missing data on a variable is replacing that missing by a value that is drawn from an estimate of the distribution of this variable. In single imputation, only one estimate is used. In multiple imputation, various estimates are used, reflecting the uncertainty in the estimation of this distribution. Under the general conditions of so-called missing at random and missing completely at random, both single and multiple imputations result in unbiased estimates of study associations. But single imputation results in too small estimated standard errors, whereas multiple imputation results in correctly estimated standard errors and confidence intervals. In this article we explain why all this is the case, and use a simple simulation study to demonstrate our explanations. We also explain and illustrate why two frequently used methods to handle missing data, i.e., overall mean imputation and the missing-indicator method, almost always result in biased estimates. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Donders, A. Rogier T. and van der Heijden, Geert J.M.G. and Stijnen, Theo and Moons, Karel G.M.},
doi = {10.1016/j.jclinepi.2006.01.014},
file = {:C\:/Users/chris/Downloads/1-s2.0-S0895435606001971-main.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Bias,Indicator method,Missing data,Multiple imputation,Precision,Single imputation},
number = {10},
pages = {1087--1091},
pmid = {16980149},
title = {{Review: A gentle introduction to imputation of missing values}},
volume = {59},
year = {2006}
}
@article{Enders2017,
abstract = {The last 20 years has seen an uptick in research on missing data problems, and most software applications now implement one or more sophisticated missing data handling routines (e.g., multiple imputation or maximum likelihood estimation). Despite their superior statistical properties (e.g., less stringent assumptions, greater accuracy and power), the adoption of these modern analytic approaches is not uniform in psychology and related disciplines. Thus, the primary goal of this manuscript is to describe and illustrate the application of multiple imputation. Although maximum likelihood estimation is perhaps the easiest method to use in practice, psychological data sets often feature complexities that are currently difficult to handle appropriately in the likelihood framework (e.g., mixtures of categorical and continuous variables), but relatively simple to treat with imputation. The paper describes a number of practical issues that clinical researchers are likely to encounter when applying multiple imputation, including mixtures of categorical and continuous variables, item-level missing data in questionnaires, significance testing, interaction effects, and multilevel missing data. Analysis examples illustrate imputation with software packages that are freely available on the internet.},
author = {Enders, Craig K.},
doi = {10.1016/j.brat.2016.11.008},
file = {:C\:/Users/chris/Downloads/1-s2.0-S0005796716301954-main.pdf:pdf},
issn = {1873622X},
journal = {Behaviour Research and Therapy},
keywords = {Attrition,Maximum likelihood estimation,Missing data,Multiple imputation},
pages = {4--18},
pmid = {27890222},
publisher = {Elsevier Ltd},
title = {{Multiple imputation as a flexible tool for missing data handling in clinical research}},
url = {http://dx.doi.org/10.1016/j.brat.2016.11.008},
volume = {98},
year = {2017}
}
@article{Richter2019,
abstract = {Traumatic brain injury (TBI) research commonly measures long-Term functional outcome, but studies often suffer from missing data as patients are lost to follow-up. This review assesses the extent and handling of missing outcome data in the TBI literature and provides a practical guide for future research. Relevant electronic databases were searched from January 1, 2012 to October 27, 2017 for TBI studies that used the Glasgow Outcome Scale or Glasgow Outcome Scale-Extended (GOS/GOSE) as an outcome measure. Studies were screened and data extracted in line with Cochrane guidance. A total of 195 studies, 21 interventional, 174 observational, with 104,688 patients were included. Using the reported follow-up rates in a mixed model, on average 91percent of patients were predicted to return to follow-up at 6 months post-injury, 84percent at 1 year, and 69percent at 2 years. However, 36percent of studies provided insufficient information to determine the number of subjects at each time-point. Of 139 studies that did report missing outcome data, only 50percent attempted to identify why data were missing, with just 4 reporting their assumption on the "missingness mechanism." The handling of missing data was heterogeneous, with the most common method being its exclusion from analysis. These results confirm substantial variability in the standard of reporting and handling of missing outcome data in TBI research. We conclude that practical guidance is needed to facilitate meaningful and accurate study interpretation, and therefore propose a framework for the handling of missing outcome data in future TBI research.},
author = {Richter, Sophie and Stevenson, Susan and Newman, Tom and Wilson, Lindsay and Menon, David K. and Maas, Andrew I.R. and Nieboer, Daan and Lingsma, Hester and Steyerberg, Ewout W. and Newcombe, Virginia F.J.},
doi = {10.1089/neu.2018.6216},
file = {:C\:/Users/chris/Downloads/neu.2018.6216.pdf:pdf},
issn = {15579042},
journal = {Journal of Neurotrauma},
keywords = {follow-up,missing data,multiple imputation,traumatic brain injury},
number = {19},
pages = {2743--2752},
pmid = {31062649},
title = {{Handling of missing outcome data in traumatic brain injury research: A systematic review}},
volume = {36},
year = {2019}
}
@article{Sterne2009,
abstract = {We are enthusiastic about the potential for multiple imputation and other methods 14 to improve the validity of medical research results and to reduce the waste of resources caused by missing data. The cost of multiple imputation analyses is small compared with the cost of collecting the data. It would be a pity if the avoidable pitfalls of multiple imputation slowed progress towards the wider use of these methods. It is no longer excusable for missing values and the reason they arose to be swept under the carpet, nor for potentially misleading and inefficient analyses of complete cases to be considered adequate. We hope that the pitfalls and guidelines discussed here will contribute to the appropriate use and reporting of methods to deal with missing data.},
author = {Sterne, Jonathan A.C. and White, Ian R. and Carlin, John B. and Spratt, Michael and Royston, Patrick and Kenward, Michael G. and Wood, Angela M. and Carpenter, James R.},
doi = {10.1136/bmj.b2393},
file = {:C\:/Users/chris/Downloads/stej610915.pdf:pdf;:C\:/Users/chris/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sterne et al. - 2009 - Multiple imputation for missing data in epidemiological and clinical research Potential and pitfalls.pdf:pdf},
issn = {17561833},
journal = {BMJ (Online)},
number = {7713},
pages = {157--160},
pmid = {19564179},
title = {{Multiple imputation for missing data in epidemiological and clinical research: Potential and pitfalls}},
volume = {339},
year = {2009}
}
@article{VanderHeijden2006,
abstract = {Background and Objectives: To illustrate the effects of different methods for handling missing data-complete case analysis, missing-indicator method, single imputation of unconditional and conditional mean, and multiple imputation (MI)-in the context of multivariable diagnostic research aiming to identify potential predictors (test results) that independently contribute to the prediction of disease presence or absence. Methods: We used data from 398 subjects from a prospective study on the diagnosis of pulmonary embolism. Various diagnostic predictors or tests had (varying percentages of) missing values. Per method of handling these missing values, we fitted a diagnostic prediction model using multivariable logistic regression analysis. Results: The receiver operating characteristic curve area for all diagnostic models was above 0.75. The predictors in the final models based on the complete case analysis, and after using the missing-indicator method, were very different compared to the other models. The models based on MI did not differ much from the models derived after using single conditional and unconditional mean imputation. Conclusion: In multivariable diagnostic research complete case analysis and the use of the missing-indicator method should be avoided, even when data are missing completely at random. MI methods are known to be superior to single imputation methods. For our example study, the single imputation methods performed equally well, but this was most likely because of the low overall number of missing values. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {van der Heijden, Geert J.M.G. and {T. Donders}, A. Rogier and Stijnen, Theo and Moons, Karel G.M.},
doi = {10.1016/j.jclinepi.2006.01.015},
file = {:C\:/Users/chris/Downloads/1-s2.0-S0895435606001983-main.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Bias,Complete case analysis,Indicator method,Missing data,Multiple imputation,Precision,Single imputation},
number = {10},
pages = {1102--1109},
pmid = {16980151},
title = {{Imputation of missing values is superior to complete case analysis and the missing-indicator method in multivariable diagnostic research: A clinical example}},
volume = {59},
year = {2006}
}
@article{Spratt2010,
abstract = {Multiple imputation is increasingly recommended in epidemiology to adjust for the bias and loss of information that may occur in analyses restricted to study participants with complete data ("complete-case analyses"). However, little guidance is available on applying the method, including which variables to include in the imputation model and the number of imputations needed. Here, the authors used multiple imputation to analyze the prevalence of wheeze among 81-month-old children in the Avon Longitudinal Study of Parents and Children (Avon, United Kingdom; 1991-1999) and the association of wheeze with gender, maternal asthma, and maternal smoking. The authors examined how inclusion of different types of variables in the imputation model affected point estimates and precision, and assessed the impact of number of imputations on Monte Carlo variability. Inclusion of variables associated with the outcome in the imputation model increased odds ratios and reduced standard errors. When only 5 or 10 imputations were used, variability due to the imputation procedure was substantial enough to affect conclusions. Careful preliminary analysis identified the scope for multiple imputation to reduce bias and improve efficiency and provided guidance for building the imputation model. When data are missing, such preliminary analyses should be routinely undertaken and reported, regardless of whether multiple imputation is used in the final analysis. {\textcopyright} 2010 The Author.},
author = {Spratt, Michael and Carpenter, James and Sterne, Jonathan A.C. and Carlin, John B. and Heron, Jon and Henderson, John and Tilling, Kate},
doi = {10.1093/aje/kwq137},
file = {:C\:/Users/chris/Downloads/kwq137.pdf:pdf},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {imputation,longitudinal studies,missing data},
number = {4},
pages = {478--487},
pmid = {20616200},
title = {{Strategies for multiple imputation in longitudinal studies}},
volume = {172},
year = {2010}
}
@article{Tilling2016,
abstract = {Objective Missing data are a pervasive problem, often leading to bias in complete records analysis (CRA). Multiple imputation (MI) via chained equations is one solution, but its use in the presence of interactions is not straightforward. Study Design and Setting We simulated data with outcome Y dependent on binary explanatory variables X and Z and their interaction XZ. Six scenarios were simulated (Y continuous and binary, each with no interaction, a weak and a strong interaction), under five missing data mechanisms. We use directed acyclic graphs to identify when CRA and MI would each be unbiased. We evaluate the performance of CRA, MI without interactions, MI including all interactions, and stratified imputation. We also illustrated these methods using a simple example from the National Child Development Study (NCDS). Results MI excluding interactions is invalid and resulted in biased estimates and low coverage. When XZ was zero, MI excluding interactions gave unbiased estimates but overcoverage. MI including interactions and stratified MI gave equivalent, valid inference in all cases. In the NCDS example, MI excluding interactions incorrectly concluded there was no evidence for an important interaction. Conclusions Epidemiologists carrying out MI should ensure that their imputation model(s) are compatible with their analysis model.},
author = {Tilling, Kate and Williamson, Elizabeth J. and Spratt, Michael and Sterne, Jonathan A.C. and Carpenter, James R.},
doi = {10.1016/j.jclinepi.2016.07.004},
file = {:C\:/Users/chris/Downloads/PIIS0895435616301986.pdf:pdf},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {Bias,Complete case analysis,Interaction,Missing data,Multiple imputation,Simulation},
pages = {107--115},
pmid = {27445178},
publisher = {Elsevier Inc},
title = {{Appropriate inclusion of interactions was needed to avoid bias in multiple imputation}},
url = {http://dx.doi.org/10.1016/j.jclinepi.2016.07.004},
volume = {80},
year = {2016}
}
@article{Graham2007,
abstract = {Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information ($\gamma$) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which $\gamma$ and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on $\gamma$, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations. {\textcopyright} 2007 Society of Prevention Research.},
author = {Graham, John W. and Olchowski, Allison E. and Gilreath, Tamika D.},
doi = {10.1007/s11121-007-0070-9},
file = {:C\:/Users/chris/Downloads/Graham2007_Article_HowManyImputationsAreReallyNee.pdf:pdf},
issn = {13894986},
journal = {Prevention Science},
keywords = {Full information maximum likelihood,Missing data,Multiple imputation,Number of imputations,Statistical power},
number = {3},
pages = {206--213},
pmid = {17549635},
title = {{How many imputations are really needed? Some practical clarifications of multiple imputation theory}},
volume = {8},
year = {2007}
}
@article{Young-Saver2018,
abstract = {Background: In acute stroke randomized trials, missingness of final functional outcome data reduces study power and potentially biases findings of treatment effect. Best methods for handling missing outcome data have not been well delineated for diseases with monophasic onset and subsequent improvement, like acute stroke. Methods: We simulated data missingness in the public dataset of the landmark, second NINDS-tPA trial, by randomly removing 5percent-25percent of actual values for the 3-month modified Rankin Scale (mRS) of global disability. We evaluated 5 missing data-handling methods: complete case analysis (CCA), worst case imputation (WCI), last observation carried forward (LOCF), multiple imputation using baseline covariates only (MI-B), and multiple imputation using baseline and postbaseline observations (MI-BP). Results: With the original trial's 333 patients, tissue plasminogen activator was associated with 3-month disability benefit, both for mRS dichotomized at 0-1 (P =.014) and shift analysis (P =.035). Distance (root mean square error) of imputed from actual mRS values was best for LOCF (1.17) and MI-BP (1.28), intermediate for MI-B (1.89) and worst for WCI (3.77). Directional bias (mean difference) was least for MI-BP (.01) and MI-B (−.16), intermediate for LOCF (−.37), and worst for WCI (−3.22). Preservation of formally positive results was greatest for MI-BP and LOCF (preserved at all missingness rates), intermediate for CCA and MI-B (preserved only with missingness <10percent-20percent), and least for WCI (preserved only with missingness <5percent-20percent). Conclusions: For acute stroke trials, multiple imputation using baseline and postbaseline observations is an advantageous approach to missing outcome data-handling, yielding high accuracy, reduced directional bias, and greater preservation of study power.},
author = {Young-Saver, Dashiell F. and Gornbein, Jeffrey and Starkman, Sidney and Saver, Jeffrey L.},
doi = {10.1016/j.jstrokecerebrovasdis.2018.08.040},
file = {:C\:/Users/chris/Downloads/1-s2.0-S105230571830497X-main.pdf:pdf},
issn = {15328511},
journal = {Journal of Stroke and Cerebrovascular Diseases},
keywords = {Acute stroke,IV-tPA,last observation carried forward,missing data,multiple imputation},
number = {12},
pages = {3662--3669},
pmid = {30297167},
publisher = {Elsevier Inc.},
title = {{Handling of Missing Outcome Data in Acute Stroke Trials: Advantages of Multiple Imputation Using Baseline and Postbaseline Variables}},
url = {https://doi.org/10.1016/j.jstrokecerebrovasdis.2018.08.040},
volume = {27},
year = {2018}
}
@article{Madley-Dowd2019,
abstract = {Objectives: Researchers are concerned whether multiple imputation (MI) or complete case analysis should be used when a large proportion of data are missing. We aimed to provide guidance for drawing conclusions from data with a large proportion of missingness. Study Design and Setting: Via simulations, we investigated how the proportion of missing data, the fraction of missing information (FMI), and availability of auxiliary variables affected MI performance. Outcome data were missing completely at random or missing at random (MAR). Results: Provided sufficient auxiliary information was available; MI was beneficial in terms of bias and never detrimental in terms of efficiency. Models with similar FMI values, but differing proportions of missing data, also had similar precision for effect estimates. In the absence of bias, the FMI was a better guide to the efficiency gains using MI than the proportion of missing data. Conclusion: We provide evidence that for MAR data, valid MI reduces bias even when the proportion of missingness is large. We advise researchers to use FMI to guide choice of auxiliary variables for efficiency gain in imputation analyses, and that sensitivity analyses including different imputation models may be needed if the number of complete cases is small.},
author = {Madley-Dowd, Paul and Hughes, Rachael and Tilling, Kate and Heron, Jon},
doi = {10.1016/j.jclinepi.2019.02.016},
file = {:C\:/Users/chris/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Madley-Dowd et al. - 2019 - The proportion of missing data should not be used to guide decisions on multiple imputation.pdf:pdf},
issn = {18785921},
journal = {Journal of Clinical Epidemiology},
keywords = {ALSPAC,Bias,Methods,Missing data,Multiple imputation,Simulation},
pages = {63--73},
pmid = {30878639},
publisher = {Elsevier Inc},
title = {{The proportion of missing data should not be used to guide decisions on multiple imputation}},
url = {https://doi.org/10.1016/j.jclinepi.2019.02.016},
volume = {110},
year = {2019}
}
@article{Lee2014,
abstract = {Missing data are common in both observational and experimental studies. Multiple imputation (MI) is a two-stage approach where missing values are imputed a number of times using a statistical model based on the available data and then inference is combined across the completed datasets. This approach is becoming increasingly popular for handling missing data. In this paper, we introduce the method of MI, as well as a discussion surrounding when MI can be a useful method for handling missing data and the drawbacks of this approach. We illustrate MI when exploring the association between current asthma status and forced expiratory volume in 1 s after adjustment for potential confounders using data from a population-based longitudinal cohort study. {\textcopyright} 2013 The Authors. Respirology {\textcopyright} 2013 Asian Pacific Society of Respirology.},
author = {Lee, Katherine J. and Simpson, Julie A.},
doi = {10.1111/resp.12226},
file = {:C\:/Users/chris/Downloads/resp.12226.pdf:pdf},
issn = {13237799},
journal = {Respirology},
keywords = {experimental study,missing data,multiple imputation,observational study},
number = {2},
pages = {162--167},
pmid = {24372814},
title = {{Introduction to multiple imputation for dealing with missing data}},
volume = {19},
year = {2014}
}
@article{Cro2020,
abstract = {Missing data due to loss to follow-up or intercurrent events are unintended, but unfortunately inevitable in clinical trials. Since the true values of missing data are never known, it is necessary to assess the impact of untestable and unavoidable assumptions about any unobserved data in sensitivity analysis. This tutorial provides an overview of controlled multiple imputation (MI) techniques and a practical guide to their use for sensitivity analysis of trials with missing continuous outcome data. These include $\delta$- and reference-based MI procedures. In $\delta$-based imputation, an offset term, $\delta$, is typically added to the expected value of the missing data to assess the impact of unobserved participants having a worse or better response than those observed. Reference-based imputation draws imputed values with some reference to observed data in other groups of the trial, typically in other treatment arms. We illustrate the accessibility of these methods using data from a pediatric eczema trial and a chronic headache trial and provide Stata code to facilitate adoption. We discuss issues surrounding the choice of $\delta$ in $\delta$-based sensitivity analysis. We also review the debate on variance estimation within reference-based analysis and justify the use of Rubin's variance estimator in this setting, since as we further elaborate on within, it provides information anchored inference.},
author = {Cro, Suzie and Morris, Tim P. and Kenward, Michael G. and Carpenter, James R.},
doi = {10.1002/sim.8569},
file = {:D\:/repository/seminararbeit/papers/Suzie Cro.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {clinical trials,controlled multiple imputation,missing data,multiple imputation,sensitivity analysis},
number = {21},
pages = {2815--2842},
pmid = {32419182},
title = {{Sensitivity analysis for clinical trials with missing continuous outcome data using controlled multiple imputation: A practical guide}},
volume = {39},
year = {2020}
}


@book{McConnell:2004:CCS:1096143,
	author = {McConnell, Steve},
	title = {Code Complete, Second Edition},
	year = {2004},
	isbn = {0735619670},
	publisher = {Microsoft Press},
	address = {Redmond, WA, USA},
}

@book{Vandevoorde:2002,
	author = {Vandevoorde, David and Josuttis, Nicolai M.},
	day = {22},
	isbn = {0201734842},
	month = {November},
	priority = {2},
	publisher = {Addison-Wesley Professional},
	title = {{C++ Templates: The Complete Guide}},
	year = {2002}
}

@inproceedings{Mulloni:2010,
 author = {Mulloni, Alessandro and D\"{u}nser, Andreas and Schmalstieg, Dieter},
 title = {Zooming Interfaces for Augmented Reality Browsers},
 booktitle = {Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services},
 series = {MobileHCI '10},
 year = {2010},
  location = {Lisbon, Portugal},
  doi = {10.1145/1851600.1851629},
  isbn = {978-1-60558-835-3},
 pages = {161--170},
 numpages = {10},  
 acmid = {1851629},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile augmented reality, zooming interfaces},
} 

@article{Albanese2004,
abstract = {The first step in a video indexing process is the segmentation of videos into meaningful parts called shots. In this paper we present a formal model of the video shot segmentation process. Starting from a mathematical characterization of the most common transition effects, a video segmentation algorithm capable to detect both abrupt and gradual transitions is proposed. The proposed algorithm is based on the computation of an arbitrary similarity measure between consecutive frames of a video. The algorithm has been tested adopting a similarity metric based on the Animate Vision theory and results have been reported.},
author = {Albanese, Massimiliano and Chianese, Angelo and Moscato, Vincenzo and Sansone, Lucio},
journal = {Multimedia Tools and Applications},
pages = {253--272},
volume={24},
number={3},
title = {{A Formal Model for Video Shot Segmentation and its Application via Animate Vision}},
year = {2004}
}


@inproceedings{Bosch2014,
abstract = {Cascading Style Sheets (CSS) is a standard language for stylizing and formatting web documents. Its role in web user experience becomes increasingly important. However, CSS files tend to be designed from a result-driven point of view, without much attention devoted to the CSS file structure as long as it produces the desired results. Furthermore, the rendering intended in the browser is often checked and debugged with a document instance. Style sheets normally apply to a set of documents, therefore modifications added while focusing on a particular instance might affect other documents of the set. We present a first prototype of static CSS semantical analyzer and optimizer that is capable of automatically detecting and removing redundant property declarations and rules. We build on earlier work on tree logics to locate redundancies due to the semantics of selectors and properties. Existing purely syntactic CSS optimizers might be used in conjunction with our tool, for performing complementary (and orthogonal) size reduction, toward the common goal of providing smaller and cleaner CSS files.},
author = {Bosch, Mart{\'{i}} and Genev{\`{e}}s, Pierre and Laya{\"{i}}ida, Nabil},
doi = {10.1145/2644866.2644885},
isbn = {9781450329491},
journal = {Proceedings of the 2014 ACM Symposium on Document Engineering},
keywords = {css,debugging,style sheets,web development},
pages = {13--16},
title = {{Automated refactoring for size reduction of CSS style sheets}},
year = {2014}
}


//Samples
@article{Fried2008,
abstract = {Recently, a debate has begun over whether in-class laptops aid or hinder learning. While some research demonstrates that laptops can be an important learning tool, anecdotal evidence suggests more and more faculty are banning laptops from their classrooms because of perceptions that they distract students and detract from learning. The current research examines the nature of in-class laptop use in a large lecture course and how that use is related to student learning. Students completed weekly surveys of attendance, laptop use, and aspects of the classroom environment. Results showed that students who used laptops in class spent considerable time multitasking and that the laptop use posed a significant distraction to both users and fellow students. Most importantly, the level of laptop use was negatively related to several measures of student learning, including self-reported understanding of course material and overall course performance. The practical implications of these findings are discussed.},
author = {Fried, Carrie B.},
journal = {Computers {\&} Education},
keywords = {Academic Achievement,Attention,Classroom Environment,Classroom Research,College Students,Computers,Student Attitudes,Student Behavior,Student Surveys,Time on Task},
mendeley-groups = {Education/Computer Science Education},
number = {3},
pages = {906--914},
pmid = {28800271},
title = {{In-class laptop use and its effects on student learning}},
volume = {50},
year = {2008}
}

@techreport{RFC2616,
author = {Fielding, R. and Gettys, James and Mogul, J. and Frystyk, H. and Masinter, Larry and Leach, P. and Berners-Lee, Tim},
doi = {10.17487/rfc2616},
howpublished = {Internet Requests for Comments},
institution = {RFC Editor},
month = {jun},
number = {2616},
publisher = {RFC Editor},
title = {{Hypertext Transfer Protocol -- HTTP/1.1}},
type = {RFC},
url = {https://www.rfc-editor.org/info/rfc2616},
year = {1999}
}
